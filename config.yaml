# engram3/config.yaml
# Configuration file for keyboard layout preference learning system
#
# Core components:
# - data: Dataset and layout configuration
# - features: Feature selection and interaction settings
# - model: Bayesian model parameters and MCMC settings
# - recommendations: Bigram pair recommendation parameters
# - visualization: Plot configuration
# - logging: System logging settings
#
# The system supports three operational modes:
# 1. Feature selection: Identifies optimal feature set
# 2. Model training: Trains preference model
# 3. Recommendations: Generates bigram pairs for data collection
#
# Each section includes detailed parameter documentation and
# typical values/ranges where appropriate.

#-----------------------------------------------------------------------
# Input data and key layout
#-----------------------------------------------------------------------
data:                                   # Path to processed preference data
  input_file: "data/input/processed_bigram_data_406participants_0inconsistencies.csv"
  splits:
    test_ratio: 0.2                     # Proportion of data used for testing (0.2 = 20%)
    random_seed: 42                     # Seed for reproducible train/test splits
    split_data_file: "data/split_train_test_indices.npz"  # File to store train/test indices
  layout:
    chars: [q, w, e, r, t,              # Characters in keyboard layout (half-QWERTY)
            a, s, d, f, g,              # (only left-hand side used for symmetric analysis)
            z, x, c, v, b]

#-----------------------------------------------------------------------
# Feature selection
#-----------------------------------------------------------------------
# The feature selection process evaluates feature importance based on:
# - model_effect (weight magnitude)
# - effect_consistency (stability across CV splits)
# - predictive_power (improvement over baseline)
# When a 3-way interaction is evaluated, it needs to provide sufficient additional predictive power, etc. 
# compared to a model that already contains all previously selected features, including 2-way interactions.
# In the round-robin feature selection process, we're making relative comparisons between features to select 
# the best one at each round. Therefore high precision MCMC settings (chains, warmup, n_samples) are less critical 
because:
# - We only need enough precision to reliably determine which feature is better
# - We're not using the absolute values for final inference
# - The relative ranking is more important than precise effect estimates
# Model settings like adapt_delta and max_treedepth can be more relaxed because:
# - Small inaccuracies won't affect the relative comparisons much
# - We're interested in "better vs worse" rather than exact effect sizes
# - Performance (speed) becomes more important since we're doing many comparisons
feature_selection:
  metric_weights:                       # Weights for combining different metrics (must sum to 1): prioritize stable, predictive features
    effect_consistency: 0.5             # Weight for consistency of effect across splits (cross-validation stability)
    predictive_power: 0.3               # Weight for prediction improvement
    model_effect: 0.2                   # Weight for feature's effect in model
  thresholds:                           # Selection criteria thresholds
    effect_consistency: 0.2             # Minimum effect consistency (0.2 from, e.g., 60% variation in magnitude and same direction in 3/5 folds)
    predictive_power: 0.05              # Minimum improvement over baseline (0.05 represents 2.5% accuracy improvement)
    model_effect: 0.001                 # Minimum normalized effect size (too small risks including numerical noise)
  min_metrics_passed: 3                 # Number of metrics that must pass thresholds
  metrics_file: "output/data/feature_metrics.csv"   # File to store feature metrics
  model_file: "output/data/feature_selection_model.pkl"  # File to store feature selection model

features:
  base_features:                        # Core features for analysis
    - typing_time                       # Time taken to type bigram
    - same_finger                       # Whether bigram uses same finger
    - sum_finger_values                 # Combined finger load values
    - adj_finger_diff_row               # Adjacent fingers in different rows
    - rows_apart                        # Number of rows between keys
    - angle_apart                       # Angular distance between keys
    - outward_roll                      # Whether movement rolls outward
    - middle_column                     # Keys in middle column (outside home columns)
    - sum_engram_position_values        # Combined Engram position values
    - sum_row_position_values           # Combined row position values
  interactions:
    - [same_finger, sum_finger_values] 
    - [same_finger, rows_apart]
    - [same_finger, sum_row_position_values]
    - [sum_finger_values, adj_finger_diff_row]
    - [sum_finger_values, rows_apart]
    - [sum_finger_values, angle_apart]
    - [sum_finger_values, outward_roll]
    - [sum_finger_values, sum_engram_position_values]
    - [sum_finger_values, sum_row_position_values]
    - [adj_finger_diff_row, rows_apart]
    - [adj_finger_diff_row, angle_apart]
    - [adj_finger_diff_row, outward_roll]
    - [adj_finger_diff_row, sum_engram_position_values]
    - [adj_finger_diff_row, sum_row_position_values]
    - [rows_apart, outward_roll]
    - [rows_apart, sum_engram_position_values]
    - [rows_apart, sum_row_position_values]
    - [angle_apart, outward_roll]
    - [angle_apart, sum_engram_position_values]
    - [outward_roll, sum_engram_position_values]
    - [outward_roll, sum_row_position_values]

    #- [same_finger, sum_finger_values, rows_apart]
    #- [same_finger, sum_finger_values, sum_row_position_values]
    ##- [same_finger, rows_apart, sum_row_position_values]
    #- [sum_finger_values, adj_finger_diff_row, rows_apart]
    #- [sum_finger_values, adj_finger_diff_row, angle_apart]
    #- [sum_finger_values, adj_finger_diff_row, outward_roll]
    #- [sum_finger_values, adj_finger_diff_row, sum_engram_position_values]
    #- [sum_finger_values, adj_finger_diff_row, sum_row_position_values]
    #- [sum_finger_values, rows_apart, outward_roll]
    #- [sum_finger_values, rows_apart, sum_engram_position_values]
    #- [sum_finger_values, rows_apart, sum_row_position_values]
    #- [sum_finger_values, angle_apart, outward_roll]
    #- [sum_finger_values, angle_apart, sum_engram_position_values]
    #- [sum_finger_values, outward_roll, sum_engram_position_values]
    #- [sum_finger_values, outward_roll, sum_row_position_values]
    #- [adj_finger_diff_row, rows_apart, outward_roll]
    #- [adj_finger_diff_row, rows_apart, sum_engram_position_values]
    #- [adj_finger_diff_row, rows_apart, sum_row_position_values]
    #- [adj_finger_diff_row, angle_apart, outward_roll]
    #- [adj_finger_diff_row, angle_apart, sum_engram_position_values]
    #- [adj_finger_diff_row, outward_roll, sum_engram_position_values]
    #- [adj_finger_diff_row, outward_roll, sum_row_position_values]
    #- [rows_apart, outward_roll, sum_engram_position_values]
    #- [rows_apart, outward_roll, sum_row_position_values]
    #- [angle_apart, outward_roll, sum_engram_position_values]

    #- [same_finger, sum_finger_values, rows_apart, sum_row_position_values]
    #- [sum_finger_values, adj_finger_diff_row, rows_apart, outward_roll]
    #- [sum_finger_values, adj_finger_diff_row, rows_apart, sum_engram_position_values]
    #- [sum_finger_values, adj_finger_diff_row, rows_apart, sum_row_position_values]
    #- [sum_finger_values, adj_finger_diff_row, angle_apart, outward_roll]
    #- [sum_finger_values, adj_finger_diff_row, angle_apart, sum_engram_position_values]
    #- [sum_finger_values, adj_finger_diff_row, outward_roll, sum_engram_position_values]
    #- [sum_finger_values, adj_finger_diff_row, outward_roll, sum_row_position_values]
    #- [sum_finger_values, rows_apart, outward_roll, sum_engram_position_values]
    #- [sum_finger_values, rows_apart, outward_roll, sum_row_position_values]
    #- [sum_finger_values, angle_apart, outward_roll, sum_engram_position_values]
    #- [adj_finger_diff_row, rows_apart, outward_roll, sum_engram_position_values]
    #- [adj_finger_diff_row, rows_apart, outward_roll, sum_row_position_values]
    #- [adj_finger_diff_row, angle_apart, outward_roll, sum_engram_position_values]
  control_features:                     # Features to control for but not analyze
    - bigram_frequency                  # Bigram frequency in the English language

#-----------------------------------------------------------------------
# Bigram pair recommendations
#-----------------------------------------------------------------------
recommendations:
  weights:                              # Weights for recommendation scoring (must sum to 1)
    prediction_uncertainty: 0.25        # Weight for model prediction uncertainty (prioritize pairs where model is uncertain)
    comfort_uncertainty: 0.25           # Weight for comfort score uncertainty (focus on uncertain comfort predictions)
    feature_space: 0.25                 # Weight for coverage of feature space (ensure good coverage of feature space)
    transitivity: 0.15                  # Weight for transitivity validation (Bradley-Terry models assume transitivity in preferences)
    stability: 0.10                     # Weight for feature stability
  n_recommendations: 50                 # Number of bigram pairs to recommend
  max_candidates: 5000                  # Maximum number of candidate pairs to evaluate
  recommendations_file: "output/data/recommended_bigram_pairs.csv"    # File to store recommendations

#-----------------------------------------------------------------------
# Model training
#-----------------------------------------------------------------------
# Settings to debug the model
__model_fast: &debug  
  chains: 1                             # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 200                           # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 500                        # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 1                      # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.80                     # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

# Settings to select features (takes days on an M1/M2 Macbook Pro)
__model_feature_selection: &feature_selection
  chains: 2                             # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 1000                          # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 3000                       # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 10                     # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.90                     # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

# Settings to train the model on the selected features
__model_slow: &model_train
  chains: 4                             # Number of parallel MCMC chains for sampling (higher for better convergence diagnostics)
  warmup: 3000                          # Number of warmup samples per chain for adaptation to parameter space (discarded) (usually 25-50% of total iterations = chains x n_samples)
  n_samples: 7000                       # Number of post-warmup samples per chain (kept) (higher for more precise posterior estimates)
  max_treedepth: 15                     # Maximum depth of NUTS sampling tree (higher = more exhaustive exploration; leapfrog steps = 2^max_treedepth)
  adapt_delta: 0.95                     # Target acceptance rate (higher = more careful sampling, fewer divergent transitions)

model:                                  # Bayesian model parameters (Stan)
  #<<: *debug                            # Model debug settings
  <<: *feature_selection                # Model feature selection settings
  #<<: *model_train                      # Model training settings
  feature_scale: 1.5                    # Scale parameter for feature weight priors (lower for more regularization)
  participant_scale: 0.5                # Scale parameter for participant effect priors (lower to reduce overfitting to individual participants)
  required_temp_mb: 2000                # Required temporary space in MB
  predictions_file: "output/data/estimated_bigram_scores.csv"   # File to store predicted comfort scores
  model_file: "output/data/bigram_score_prediction_model.pkl"   # File to store trained model

#-----------------------------------------------------------------------
# Visualization, logging, and paths
#-----------------------------------------------------------------------
visualization:
  dpi: 300                              # Resolution of saved plots (dots per inch)
  figure_size: [12, 8]                  # Default figure dimensions (width, height)
  alpha: 0.6                            # Transparency for plot elements
  color_map: "viridis"                  # Default colormap for visualizations (viridis is perceptually uniform)

logging:                                # Logging configuration
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"  # Standard Python logging format
  console_level: "INFO"                 # Show INFO and above messages in console
  file_level: "DEBUG"                   # Store DEBUG and above messages in log file

paths:                                  # Directory structure for outputs
  root_dir: "output"                    # Base directory for all outputs
  metrics_dir: "output/data"            # Directory for storing computed metrics
  plots_dir: "output/plots"             # Directory for saving visualizations
  logs_dir: "output/logs"               # Directory for log files
  stan_temp: "output/stan_temp"         # Directory for Stan temporary files